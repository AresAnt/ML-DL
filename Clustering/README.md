# 一些聚类算法

标签（空格分隔）： Birch层次聚类 KMeans原形算法 AGNES层次算法 DBSCAN密度算法 LVQ原形算法

---
简单做一个个人想法记录，其实聚类算法来说大体上的思路都非常简单，无非是找点或者区域，计算它们之间的距离（这个距离为抽象意义上的距离，不同的内容有不同的距离表示方式），然后根据一定规则（一般都是以最近为相邻），然后叠加在一起。
简单来说：聚类就是如何确认某些点或区域，再根据这些选好的点与区域再进行距离计算，从而进行划分。
简单代码可见：https://github.com/AresAnt/ML-DL

Kmeans算法：（原形聚类方法）
---------
最简单的Kmeans算法的思路，首先我们要确定划分几个类（即K值）。
假设这里需要划分3个类：

 1. 先初始化三个随机样本点。（随机点一定是要在样本范围之内，一般来说可以以随机挑选三个样本点作为起始样本点）
 2. 对全样本进行遍历，与三个随机点进行计算，与哪个随机点近就划分进入哪个随机点的区域内。
 3. 一次划分完毕后，对区域内所有的点间距离进行平均计算。假设当前区域内划分进来了m-1个样本点，加上随机点，总共为m个样本在这片区域内。两两计算它们之间的距离，总共需要循环计算 m *（m-1） 次，然后除以这片区域内的样本数量。即
$$ {\frac{1}{m}}*{\sum^{m}_{i=1,i<j}λ_iλ_j}$$
 4. 算出后的点，即为新的随机样本点（也称为算质心），然后重复以上的过程，不断的更替质心直到最后确定。


LVQ算法：（原形聚类方法）
--------------
LVQ算法是假设数据样本带有类别标记的，学习过程中利用样本的这些监督信息来辅助聚类。
（简单描述，其实它的做法与KMeans类似，也是相当于是一个找质心点的过程，这里为找原形向量，每一行的向量其实对应的就是上述所提到的质心点）

算法流程：
输入：样本集D = {（x1,y1）,(x2,y2),....,(xm,ym)} (设 D1 = (x1,y1))
      原形向量的个数q，各原形向量的类别标记 { t1,t2,t3,t4,..,tq } 【这个划分的意思是指，假设我上述样本集中的 yi 为两类，即 0，1, 那么我可以设置原形向量的类别为 { 1,1,0,0,1} (意思为1分类的样本最后会划分为3个簇，0分类的样本最后会划分为2个簇】
      学习率 α （0~1）

过程：

 1. 初始化一组原形向量 P = {（p1,t1）,(p2,t2) ,...,(pq,tq)} (这个初始化一般以随机改 ti
    分类下的样本作为 pi)
```python
        # 初始化原形向量，这里做简单操作，假设希望找到 3个好瓜的簇，2个坏瓜的簇，总共五个簇
        good = np.where(rowdata_y == 1)[0]
        bad = np.where(rowdata_y == 0)[0]
        vectorlist = random.sample(list(good),3) + random.sample(list(bad),2)

        P_x = np.zeros((k,rowdata_x.shape[1]))
        P_y = np.zeros((k,rowdata_y.shape[1]))

        for i in range(len(vectorlist)):
            P_x[i] = rowdata_x[vectorlist[i]]
            P_y[i] = rowdata_y[vectorlist[i]]
```

 2. 循环迭代：
        从样本 D 中随机抽取一个样本（xi,yi）进行向量校正（类似前面的移动质心点）
        找出最小的原形向量 pj

```python
    if D.yi == P.tj:
        P.pj = P.pj + α * ( D.xi - P.pj )
    else:
        P.pj = P.pj - α * ( D.xi - P.pj )
```

 3. 输出原形向量 P

缺点：LVQ算法的质心点移动取决于进来计算的随机点，这个点的进来顺序有关，如果进来的点都过于的奇特，那么质心点的改变也会特别的奇怪。需要一定量的迭代次数来进行更正。不过因为随机点是随机产生，随机产生符合正太分布，正态分布的点数为样本中心点质量有关。

AGNES 层次聚类算法：（层次聚类算法）
---------------------
层次聚类算法可以有“自底向上”的聚合策略，也可以采用“自顶向下”的分拆策略。(2分-Kmeans就是“自顶向下”的策略）
AGNES是“自底向上”的聚合策略，它简单的先将每个样本点看做是一个簇，然后通过计算相应的距离，选取最小的两个簇合成为以个簇，以此反复，计算量庞大。

算法思路，对于数据集D，D={x_1,x_2,…,x_n}：

将数据集中的每个对象生成一个簇，得到簇列表C，C={c_1,c_2,…,c_n}
a) 每个簇只包含一个数据对象：c_i={x_i}；

重复如下步骤，直到C中只有一个簇：
a) 从C中的簇中找到两个“距离”最近的两个簇：min⁡〖D(c_i,c_j)〗；
b) 合并簇c_i和cj，形成新的簇c(i+j)；
c) 从C中删除簇c_i和cj，添加簇c(i+j)

稍微注意一下簇间距离运算的方式：
在上面描述的算法中涉及到计算两个簇之间的距离，对于簇C_1和C_2，计算〖D(C_1,C〗_2)，有以下几种计算方式：

单连锁（Single link）：
![此处输入图片的描述][1]
![此处输入图片的描述][2]
两个簇之间最近的两个点的距离作为簇之间的距离，该方式的缺陷是受噪点影响大，容易产生长条状的簇。

全连锁（Complete link）
![此处输入图片的描述][3]
![此处输入图片的描述][4]
两个簇之间最远的两个点的距离作为簇之间的距离，采用该距离计算方式得到的聚类比较紧凑。

平均连锁（Average link）
![此处输入图片的描述][5]
![此处输入图片的描述][6]
两个簇之间两两点之间距离的平均值，该方式可以有效地排除噪点的影响。

Birch层次聚类算法：（层次聚类算法）
------------

不做多余赘述，可以查看该链接：https://www.zybuluo.com/aresant/note/959282


  [1]: https://blog-10039692.file.myqcloud.com/1496651613755_1440_1496651614029.png
  [2]: https://blog-10039692.file.myqcloud.com/1496651705805_9954_1496651705880.png
  [3]: https://blog-10039692.file.myqcloud.com/1496651642287_3394_1496651642265.png
  [4]: https://blog-10039692.file.myqcloud.com/1496651676140_1786_1496651676101.png
  [5]: https://blog-10039692.file.myqcloud.com/1496651752918_3056_1496651752896.png
  [6]: https://blog-10039692.file.myqcloud.com/1496651771338_4445_1496651771308.png